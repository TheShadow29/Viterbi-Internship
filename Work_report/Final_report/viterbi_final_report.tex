\documentclass{article}
\usepackage[a4paper, tmargin=1in, bmargin=1in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{parskip}
\usepackage{pdflscape}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}

% \usepackage{titlesec}

\newcommand{\ra}{$\rightarrow$}
% \usepackage[
%     backend=biber,
%     style=authoryear,
%     maxcitenames=2,
%     sorting=nyt,
%     backref=true
%     ]{biblatex}
%     \addbibresource{ref.bib}

\title{Viterbi Internship - Final Work Report}
% \author{Arka Sadhu}
\author{Arka Sadhu\\{ Supervised by: Prof. Ram Nevatia}}

\date{\today}

\begin{document}
\maketitle

\tableofcontents
\newpage

\section{Abstract}
Media forensics in general involves detection of the tampered media, identification of the tampered portion as well as trying to recover the original media.

\section{Introduction}
The work is done as a part of the MediFor Project. The MediFor project aims at pushing the state of the art research in the field of media forensics which in broad sense deals with the tampering of the media (image, video or audio) and its detection. This work only deals with image forensics. For each manipulated image the MediFor project demands the actual image on which manipulation is done (this is called the baseline image), the kind of manipulation, and in case of splice manipulation where one image is spliced onto another image it also demands the donor image. This work focuses only on the first part, where the aim is to find the baseline image. It is assumed that the world set contains the true baseline image. All experiments are done on Nimble Dataset which is publicly available for use.

\section{Theory}
\subsection{Basic Definitions}
\begin{itemize}
\item Probe Image : This is the given image. It may or may not be manipulated.
\item Probe folder : Folder containing the probe images.
\item Base Image : This the actual image corresponding to a probe image with no manipulations exists.
\item Donor Image : In the case where the manipulation is such that a part of image A is pasted onto image B, then image A is called the Donor Image and B is the base image. The resulting image would be the manipulated image which would exist in the probe folder.
\item World folder : Folder contaning all the images. This includes base, donor as well as the probe images.
\item World set : The collection of images in the world folder. It is used interchangeably with world images.
\item Provenance : Provenance in simple sense means the origin, so it defines the original image of a particular probe image.
\item Provenance Graph : A relational graph which depicts all the transformations a particular baseline image would've undergone to reach the probe image. It is assumed that all the intermediate images are also a part of the world dataset.
\item Base detection : Detection of the base image from a given probe image and the entire world set.
\item Donor detection : Detection of the donor image from a given probe image and the entire world set.
\end{itemize}

\subsection{MediFor Project}
The MediFor project broadly has two main categories Video and Image. For any kind of media, MediFor Project wants automated assessment of the integrity of the media. If successful, the MediFor platform will automatically detect manipulations, provide detailed information about how these manipulations were performed, and reason about the overall integrity of visual media to facilitate decisions regarding the use of any questionable image or video.\cite{MedF_w}

There are three technical areas of interest for integrity analytics. \cite{MedF_dw}
\newline
----May need to add a few more lines here---------
\begin{itemize}
\item Digital Integrity : This is related to the noise modelling and statistics and its consistency.
\item Physical Integrity : This is related to shadow consistency.
\item Semantic Integrity : This is related to semantic consistency
\end{itemize}

In this work we are concerned only with semantic integrity.

\subsection{Contributions of this Work}
% ----------To Be Filled Later --------------- %

% \subsection{Datasets Used} ------- Can be included in Appendix kinda
\subsection{Base Detection and Provenance}
Base detection problem is essentially finding the underlying base image given a probe image. Here we make the assumption that the base image exists in the world set. The next problem is to get all the manipulated images derived from the base image. And beyond this is to create a provenance graph of the collected manipulated images. The last problem is not addressed in this work.

\subsubsection{Neural Networks used}
We use two pre-trained caffe \cite{jia2014caffe} models in this work. AlexNet\cite{NIPS2012_4824} trained on Places365\cite{zhou2017places} and AlexNet trained on ImageNet. The reason for using AlexNet instead of VGG16 or any other models is that we wanted to work with a simplest model and test our performances without compromising memory and time. Places365 is a scene-centric dataset while ImageNet is object centric dataset. And as such we expect there should be a difference in their base detection capability.
\begin{table}[]
\centering
\caption{Places365 Validation}
\label{p365_val}
\begin{tabular}{|l|l|ll|}
  \hline
  Correct Matches & Total Images & Accuracy  &\\
  \hline
  2975 & 3650 & 81.5068493151 &  \\
  2969 & 3650 & 81.3424657534 &  \\
  2952 & 3650 & 80.8767123288 &  \\
  2993 & 3650 & 82            &  \\
  2977 & 3650 & 81.5616438356 &  \\
  3036 & 3650 & 83.1780821918 &  \\
  2941 & 3650 & 80.5753424658 &  \\
  2976 & 3650 & 81.5342465753 &  \\
  2941 & 3650 & 80.5753424658 &  \\
  2938 & 3650 & 80.4931506849 & \\
  \hline
\end{tabular}
\end{table}

In this work we use the AlexNet trained on Places-365 everywhere unless explicitly mentioned that the AlexNet trained on ImageNet is used.

\subsubsection{Which layer and metric to choose?}
To find the baseline image, we employ the following method. We use the Nx1 dimensional vector produced by the network. As we go deeper into the layers, we expect more semantic features to be captured. The features are represented in the form of a vector and is known as a feature vector. In the AlexNet architecture we specifically compare three layers fc7, fc8, and prob layer which is the output after the operation of softmax function.

So we intend to find a way such that given the feature vectors from the probe image, we want to be find the base image. We use a simple approach for this. We find the feature vectors of the base image as well, and then compare the feature vectors using different metrics. For a metric to be good we would ideally want for a probe base pair it should give a high value and for unrelated images it should return a very low value. Also we would prefer a substantial difference between related and unrelated images. For this work we tried the following metrics :
\begin{itemize}
\item SSD : Sum of Squared Distances
\item SAD : Sum of Absolute Distances
\item NCC : Pearson's correlation coefficient
\end{itemize}

It turned out that NCC gave the most desirable results.
\begin{figure}[H]
  \centering
  \begin{subfigure}[t]{0.45\textwidth}
    \centering
    \includegraphics[scale=0.1]{images/nc17_d1b4/pb11}
  \end{subfigure}
  % ~
  \begin{subfigure}[t]{0.45\textwidth}
    \centering
    \includegraphics[scale=0.1]{images/nc17_d1b4/pb12}
  \end{subfigure}
  \caption{Probe Base Pair taken from Nimble Dataset 2017 Dev 1 Beta 4}
  \label{fig:ncc_res}
\end{figure}

For example in the image pair \ref{fig:ncc_res} the metrics for the prob layers were as follows :

\begin{table}[H]
\centering
\caption{Prob Layer Metrics}
\label{pb_metrics}
\begin{tabular}{|l|l|}
  \hline
  & Prob layer          \\
  \hline
  SSD & 0.072518922         \\
  SAD & 0.29026049          \\
  NCC & 0.98303729249860383 \\
  \hline
\end{tabular}
\end{table}

Clearly SAD is not desirable since it gives a medium score to a matching pair. Both SSD and NCC give good results in this case, but emperically it was found that NCC is not only easier for comparison (need not invert the high and low score), but is also more robust, that is gives high score even in cases where the images have been manipulated to larger degree and SSD isn't able to caputre the similarity. As a result, NCC has been the prime candidate for the rest of the work.

Another important part was to choose a layer. It was found that for many cases that using the last layer (prob) gave a very low score for a matching pair.

\begin{figure}[H]
  \centering
  \begin{subfigure}[t]{0.5\textwidth}
    \centering
    \includegraphics[scale=0.05]{images/nc17_d1b4/pb21}
  \end{subfigure}
  % ~
  \begin{subfigure}[t]{0.45\textwidth}
    \centering
    \includegraphics[scale=0.05]{images/nc17_d1b4/pb22}
  \end{subfigure}
  \caption{Probe Base Pair taken from Nimble Dataset 2017 Dev 1 Beta 4}
  \label{fig:fc8_good}
\end{figure}

For example the image pair \ref{fig:fc8_good} returns the following NCC scores for the three layers.
\begin{table}[H]
\centering
\caption{NCC scores for different layers}
\label{my-label}
\begin{tabular}{|l|l|}
  \hline
  & NCC                 \\
  \hline
  fc7  & 0.75791334934860821 \\
  fc8  & 0.87673938938958917 \\
  prob & 0.35012885670990512 \\
  \hline
\end{tabular}
\end{table}

Clearly fc8 gives the most desirable result, but it is interesting to theorize the reason why prob gives such a low score. We hypothesize that the SoftMax layer in some sense disturbs the features because it gives the probability of closeness to a particular scene. So if a scene is not present in the Places365 database, this would give a weird output. Also going by the emperical knowledge that the deeper layers tend to extract out more semantic features, fc8 should give the best result and this intuition follows our finding. fc8 gives consistently higher score than fc7 and fc6 for probe base pair and lower score for unrelated images.

Here is a small table comparing the different methods

\begin{table}[H]
\centering
\caption{Layers and Correlation threshold on the Nimble 2016 dataset which were known to be manipulated}
\label{lct}
\begin{tabular}{|l|l|l|l|l|ll|}
  \hline
  Tot Images = 320 & \multicolumn{2}{|l|}{Prob} & \multicolumn{2}{|l|}{fc8} & \multicolumn{2}{|l|}{fc7} \\
  \hline
  \multicolumn{1}{|l|}{Threshold} & correct    & \%correct   & correct   & \%correct   & correct   & \%correct   \\
  \hline
  0.95             & 203        & 0.634375    & 271       & 0.846875    & 175       & 0.546875    \\
  0.9              & 243        & 0.759375    & 288       & 0.9         & 243       & 0.759375    \\
  0.8              & 264        & 0.825       & 312       & 0.975       & 287       & 0.896875    \\
  0.5              & 295        & 0.921875    & 316       & 0.9875      & 313       & 0.978125    \\
  0.4              & 302        & 0.94375     & 320       & 1           & 316       & 0.9875      \\
  \hline

\end{tabular}
\end{table}

\subsubsection{Speeding up the Feature Extraction Process}
% Add a few lines about the multiprocessing library
The feature extraction process (for all images in the world set) can be time consuming. For this reason we used multiprocessing to spawn new processes. One process was reserved for the Caffe net, and all the other processes were used for pre-processing the images. This lead to a significant speed. For the to process 3650 passes of through the net it took 652 seconds in a standalone code, while using multiprocessing reduced it to 87 seconds.


\subsubsection{Image Slicing}
A general observation in the datasets was that the manipulation existed in only a part of the probe image. For this reason, we use the method of image slicing, that is cutting the image into two halves horizontally or vertically, even getting four quadrants as well. Then we match each slice with the corresponding slice in the other image. This gives a very easy boost to the accuracy but at the same time demands more computational resources or time.

Here is the table for layer fc8 with horizontal slicing:

\begin{table}[H]
\centering
\caption{Increase in accuracy using slicing}
\label{slicing}
\begin{tabular}{|l|l|l|}
  \hline
  Tot Images = 320 & \multicolumn{2}{|l|}{fc8 sliced} \\
  \hline
  Threshold        & correct       & \%correct      \\
  \hline
  0.95             & 203           & 0.634375       \\
  0.9              & 243           & 0.759375       \\
  0.8              & 264           & 0.825          \\
  0.5              & 295           & 0.921875       \\
  0.4              & 302           & 0.94375 \\
  \hline
\end{tabular}
\end{table}

Some problems with this method include that it is not able to identify if the image has been rotated. This also fails miserably if the donor image occupies a significant (more than 70\%) of the whole image.

\subsubsection{Clustering using K-means}
A detour atttempt was made to check how far the correlation matching could take us. We simply tried using the feature vectors derived from all the probe image which were known to be manipulated and all the world images from the Nimble 2017 Dev 1 Beta 4 dataset and tried to use a k-means clustering implemented in scikit-learn \cite{scikit-learn}. In the dataset there were 65 probe images which were known to be manipulated, and hence k in k-means was chosen to be 65. The number of iterations was kept to 100 but changing to 1000 or even higher didn't change the actual result. This table summarizes the findings :

\begin{table}[H]
\centering
\caption{Clustering observation}
\label{my-label}
\begin{tabular}{|l|l|}
  \hline
  Cluster observed    & Number of clusters \\
  \hline
  Very good              & 37                 \\
  one bad             & 10                 \\
  two bad             & 3                  \\
  two cluster overlap & 6                  \\
  bad cases           & 9                 \\
  \hline
\end{tabular}
\end{table}
Very good implies no false positives or negatives, whereas one bad and two bad imply that there is one or two false positive. Two cluster overlap implies two clusters overlapped and couldn't form distinct clusters and a possible reason this happened would be because of less number cluster centres available. Bad cases include all other cases which includes random images together, three clusters, more than two bad images etc. Here is one very good cluster

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.35]{images/cluster_1}
  \caption{Cluster 1 form NC2017 Dev 1 Beta 4 dataset}
  \label{fig:cluster}
\end{figure}

% A significant difficulty with this approach is that
\subsubsection{Getting all the Base Images}
The Nimble Dataset 2017 Dev 3 Beta 1 had a lot more probe and world images, specifically 2157 manipulated probe images and 4098 world images. The world set not only consisted of the base image, but also of the probe image as well as the intermediate manipulated images. This essentially means that there is one particular base image and then subsequent manipulations on the top of that base image gives us the probe image. We aim to find all the manipulated images along with the base image.

For this we use a graph based approach. All graphs are made using networkx \cite{hagberg-2008-exploring}. We first create a graph G with all the probe images as the node. Then we add all the images in the world (discarding the probe images) to the Graph and create an edge with all the nodes, with the weight of the edge as the correlation between the two images. We then start with one probe image, and then look at the edge with highest weight. We now contract the two nodes into one, and in this processs recompute the correlation taking the maximum of the two correlations. Then we repeat the process. The termination step is not exactly defined and for now we terminate based on the existing knowledge of the number of matches that should have occured (using the ground truth data). We then simply repeat this process for the rest of the probe images (initializing the Graph as well).

We use both Alexnet trained on Places365 as well as ImageNet. This method is henceforth referred to as recurrent base detection.

\begin{table}[H]
\centering
\caption{Recurrent Base Detection}
\label{recbd}
\begin{tabular}{|l|l|l|l|l|}
\hline
                                                                                                 & \multicolumn{2}{l|}{Alexnet on Places} & \multicolumn{2}{l|}{Alexnet on ImageNet} \\ \hline
No. of probe images                                                                              & 2157              &                    & 2157               &                     \\ \hline
\begin{tabular}[c]{@{}l@{}}No. of probe images \\ with all baseline\\ correct match\end{tabular} & 1120              &                    & 1357               &                     \\ \hline
                                                                                                 & 1120/2157         & 0.5192396847       & 1357/2157          & 0.6291145109        \\ \hline
Total no. of base images                                                                         & 56223             &                    & 56223              &                     \\ \hline
\begin{tabular}[c]{@{}l@{}}No. of base images \\ correctly identified\end{tabular}               & 48732             &                    & 49974              &                     \\ \hline
                                                                                                 & 48732/56223       & 0.8667627128       & 49974/56223        & 0.8888533163        \\ \hline
\end{tabular}
\end{table}

The first set of rows define the number of probe image with correct matches. We define a probe to be correctly matched if and only if all the manipulations were successfully captured. As can be seen this number is on the lower side. The second set of rows define the number of base images correctly recognised. That is for a particular probe image it is likely that there 7 correct images identified and 3 incorrect, even though this would make the probe image be an incorrect match, it would still be counted as 7 correct matches for the base images. We note that this is moderately on the higher side 85-89\%.

It is quite interesting to note that the AlexNet trained on ImageNet outperforms the AlexNet trained on Places365. This is probably because the manipulations in the Nimble Dataset 2017 Dev 3 Beta 1 involved small manipulations.

\subsubsection{Self Generated Images with more Manipulations}
This was done mostly as an experiment to understand if it was possible to extend the use simple NCC to bigger manipulations, which involved more than 25\% of the base image being covered by another image. For this we use scikit-image \cite{scikit-image}. We pick any two images at random one being the base other being the donor, choose an angle of rotation at random, get a portion of the donor image (or the whole donor image) with both width and height half of that of the base image, then rotate it and place it on the base image to get a new image. We create 100 such images for using images from  each of the dataset NC2017 Dev 1 and Dev 3. Now we use NCC to find the correct matches. There were few errors in processing a few of them hence the reduced number of total images.

\begin{table}[H]
\centering
\caption{NCC on NC2017 Dev 1 Beta 4}
\label{sg_dev1}
\begin{tabular}{|l|l|l|l|}
\hline
      Dev 1                         & Top1         & Top5         & Top10        \\ \hline
Places 365                     & 37/89        & 61/89        & 71/89        \\ \hline
                               & 0.4157303371 & 0.6853932584 & 0.797752809  \\ \hline
Places 365 (with 0.95 cut off) & 50/89        & 72/89        & 76/89        \\ \hline
                               & 0.5617977528 & 0.808988764  & 0.8539325843 \\ \hline
                               &              &              &              \\ \hline
bvlc AlexNet                           & 29/89        & 44/89        & 51/89        \\ \hline
                               & 0.3258426966 & 0.4943820225 & 0.5730337079 \\ \hline
bvlc AlexNet(with 0.95 cut off)       & 34/89        & 47/89        & 54/89        \\ \hline
                               & 0.3820224719 & 0.5280898876 & 0.606741573  \\ \hline
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{NCC on NC2017 Dev 3 Beta 1}
\label{sg_dev3}
\begin{tabular}{|l|l|l|l|}
\hline
Dev 3                          & Top1         & Top5         & Top10        \\ \hline
Places 365                     & 10/92        & 28/92        & 46/92        \\ \hline
                               & 0.1086956522 & 0.3043478261 & 0.5          \\ \hline
Places 365 (with 0.95 cut off) & 50/92        & 63/92        & 66/92        \\ \hline
                               & 0.5434782609 & 0.6847826087 & 0.7173913043 \\ \hline
                               &              &              &              \\ \hline
bvlc AlexNet                           & 3/92         & 17/92        & 29/92        \\ \hline
                               & 0.0326086957 & 0.1847826087 & 0.3152173913 \\ \hline
                               &              &              &              \\ \hline
bvlc AlexNet (with 0.95 cut off)       & 31/92        & 42/92        & 46/92        \\ \hline
                               & 0.3369565217 & 0.4565217391 & 0.5          \\ \hline
\end{tabular}
\end{table}

In both the tables, the top-k denotes if the actual base image is found within the top k results. Since the datasets had many similar images (images with small manipulations), we have assumed that if the image predicted and the actual image have a correlation greater 0.95 then it can be said to be a correct match.

It is quite interesting to note that Places365 does significantly better than the ImageNet. This is presumably because the Places365 inherently tries to capture the scene information rather than the object information. While it is difficult to identify an object given a small part of it, it is much easier to identify the scene even if a significant portion of it is occluded. In this sense Places365 definitely proves to be a much better candidate than the ImageNet for cases with significant amount of manipulations.

\subsubsection{ROC and CMC curves}
ROC (Receiver Operation Characteristics) and CMC (Cumulative Match Characteristics) are metrics to evaluate the performance of a system \cite{RC_lec}.
--------------Need to add a few lines of explaination-----------------------
\begin{figure}[H]
  \centering
  \begin{subfigure}[H]{1.0\linewidth}
    \centering
    \includegraphics[scale=0.3]{images/roc_cmc/only_base_ref_roc_NC17_dev1}
    \caption{ROC for NC2017 dev1}
  \end{subfigure}
  \begin{subfigure}[H]{1.0\linewidth}
    \centering
    \includegraphics[scale=0.3]{images/roc_cmc/only_base_ref_roc_NC17_dev3_take2}
    \caption{ROC for NC2017 dev3}
  \end{subfigure}
  \caption{ROC Plots}
  \label{fig:roc1}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.3]{images/roc_cmc/cmc_dev3}
  \caption{CMC for NC2017 dev3}
  \label{fig:cmc1}
\end{figure}

\subsection{Donor Detection with Protest Dataset}
This is a dataset (containing about 1971 images) created using the keyword 'Protest' in YFCC (Yahoo Flickr Creative Commons 100 Million (YFCC100m) dataset) \cite{DBLP:journals/corr/ThomeeSFENPBL15}. A few additional scenes were taken and the protest images were carefully cropped and put into the the scenes (with changes to illumination and lighting) to create 10 manipulated images.

Here is an example of the original and manipulated image \ref{fig:pr_imgs}
\begin{figure}[H]
  \centering
  \begin{subfigure}[H]{0.4\linewidth}
    \includegraphics[scale=0.1]{images/protest/pr11}
  \end{subfigure}
  ~
  \begin{subfigure}[H]{0.4\linewidth}
    \includegraphics[scale=0.25]{images/protest/pr12}
  \end{subfigure}
  \caption{Original and Manipulated Images from Protest Dataset}
  \label{fig:pr_imgs}
\end{figure}

The aim on this dataset is to try to match the donor (and not the base), i.e. donor detection. For this reason we used\cite{6126456} to get bounding boxes based on the objectness score. We note that the donor image and at least one of the top 20 bounding boxes have $(Intersection of Union)IOU > 0.5$ with probability $0.96$. We also note that if we also insert the condition that the bounding box also needs to circumscribe the donor image (perhaps for better detection) the probability reduces to $0.78$.

Again as an experiment, we used the ground truth bounding boxes and cropped out the portion which contained the donor image and tried to find the best match in the world set which consisted of all the protest images. It was quite interesting to note that the comparison using the feature vectors from Places365 were terrible and gave $0/10$ correct matches. Rather the feature vectors from Imagenet gave $7/10$ correct matches.

\section{Miscellaneous}
\subsubsection{Text Detection}
Since there was no obvious way out, we speculated that perhaps matching text would be easier rather the whole image. Also since we are dealing with protest image, we expect there to be text in the image. For text detection we directly use the code provided by the paper \cite{DBLP:journals/corr/TianHHH016}. We use text detection and not recognition because we are only interested in image matching and not in exactly what is written. The text detection works fairly well \ref{fig:pr_imgs}.

\begin{figure}[H]
  \centering
  \begin{subfigure}[H]{0.4\linewidth}
    \includegraphics[scale=0.12]{images/protest/tl21}
  \end{subfigure}
  ~
  \begin{subfigure}[H]{0.4\linewidth}
    \includegraphics[scale=0.3]{images/protest/tl22}
  \end{subfigure}
  \caption{Text Detection in Modified and Original Images}
  \label{fig:pr_imgs}
\end{figure}

But there are obvious problems with this method. There is no guarentee that the same part of the text in both images will be captured. Also this method will fail if there are no text scenes in the donor image. Also if the base image already consists many text scenes (like a city place or a mall) then there will be too many texts to be detected. Also even in the case of only donor image having text, the ordering of the bounding boxes may not be consistent, which will lead us to do a brute force search eventually.

Even then if we try to match two text images there are other problems. The illumination is different in the two images and even if histogram equalization is applied there is no benefit, the correlation more or less remains the same.



\bibliography{./ref.bib}
\bibliographystyle{ieeetr}

\end{document}