\documentclass{article}
\usepackage[a4paper, tmargin=1in, bmargin=1in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{parskip}
\usepackage{pdflscape}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}

% \usepackage{titlesec}

\newcommand{\ra}{$\rightarrow$}
% \usepackage[
%     backend=biber,
%     style=authoryear,
%     maxcitenames=2,
%     sorting=nyt,
%     backref=true
%     ]{biblatex}
%     \addbibresource{ref.bib}

\title{Viterbi Internship - Final Work Report}
% \author{Arka Sadhu}
\author{Arka Sadhu\\{ Supervised by: Prof. Ram Nevatia}}

\date{\today}

\begin{document}
\maketitle

\tableofcontents
\newpage

\section{Abstract}
Media forensics in general involves detection of the tampered media, identification of the tampered portion as well as trying to recover the original media.

\section{Introduction}
The work is done as a part of the MediFor Project. The MediFor project aims at pushing the state of the art research in the field of media forensics which in broad sense deals with the tampering of the media (image, video or audio) and its detection. This work only deals with image forensics. For each manipulated image the MediFor project demands the actual image on which manipulation is done (this is called the baseline image), the kind of manipulation, and in case of splice manipulation where one image is spliced onto another image it also demands the donor image. This work focuses only on the first part, where the aim is to find the baseline image. It is assumed that the world set contains the true baseline image. All experiments are done on Nimble Dataset which is publicly available for use.

\section{Theory}
\subsection{Basic Definitions}
\begin{itemize}
\item Probe Image : This is the given image. It may or may not be manipulated.
\item Probe folder : Folder containing the probe images.
\item Base Image : This the actual image corresponding to a probe image with no manipulations exists.
\item Donor Image : In the case where the manipulation is such that a part of image A is pasted onto image B, then image A is called the Donor Image and B is the base image. The resulting image would be the manipulated image which would exist in the probe folder.
\item World folder : Folder contaning all the images. This includes base, donor as well as the probe images.
\item World set : The collection of images in the world folder. It is used interchangeably with world images.
\item Provenance : Provenance in simple sense means the origin, so it defines the original image of a particular probe image.
\item Provenance Graph : A relational graph which depicts all the transformations a particular baseline image would've undergone to reach the probe image. It is assumed that all the intermediate images are also a part of the world dataset.
\item Base detection : Detection of the base image from a given probe image and the entire world set.
\item Donor detection : Detection of the donor image from a given probe image and the entire world set.
\end{itemize}

\subsection{MediFor Project}
The MediFor project broadly has two main categories Video and Image. For any kind of media, MediFor Project wants automated assessment of the integrity of the media. If successful, the MediFor platform will automatically detect manipulations, provide detailed information about how these manipulations were performed, and reason about the overall integrity of visual media to facilitate decisions regarding the use of any questionable image or video.\cite{MedF_w}

There are three technical areas of interest for integrity analytics. \cite{MedF_dw}
\newline
----May need to add a few more lines here---------
\begin{itemize}
\item Digital Integrity : This is related to the noise modelling and statistics and its consistency.
\item Physical Integrity : This is related to shadow consistency.
\item Semantic Integrity : This is related to semantic consistency
\end{itemize}

In this work we are concerned only with semantic integrity.

\subsection{Contributions of this Work}
% ----------To Be Filled Later --------------- %

% \subsection{Datasets Used} ------- Can be included in Appendix kinda
\subsection{Base Detection and Provenance}
Base detection problem is essentially finding the underlying base image given a probe image. Here we make the assumption that the base image exists in the world set. The next problem is to get all the manipulated images derived from the base image. And beyond this is to create a provenance graph of the collected manipulated images. The last problem is not addressed in this work.

\subsubsection{Neural Networks used}
We use two pre-trained caffe \cite{jia2014caffe} models in this work. AlexNet\cite{NIPS2012_4824} trained on Places365\cite{zhou2017places} and AlexNet trained on ImageNet. The reason for using AlexNet instead of VGG16 or any other models is that we wanted to work with a simplest model and test our performances without compromising memory and time. Places365 is a scene-centric dataset while ImageNet is object centric dataset. And as such we expect there should be a difference in their base detection capability.
\begin{table}[]
\centering
\caption{Places365 Validation}
\label{p365_val}
\begin{tabular}{|l|l|ll|}
  \hline
  Correct Matches & Total Images & Accuracy  &\\
  \hline
  2975 & 3650 & 81.5068493151 &  \\
  2969 & 3650 & 81.3424657534 &  \\
  2952 & 3650 & 80.8767123288 &  \\
  2993 & 3650 & 82            &  \\
  2977 & 3650 & 81.5616438356 &  \\
  3036 & 3650 & 83.1780821918 &  \\
  2941 & 3650 & 80.5753424658 &  \\
  2976 & 3650 & 81.5342465753 &  \\
  2941 & 3650 & 80.5753424658 &  \\
  2938 & 3650 & 80.4931506849 & \\
  \hline
\end{tabular}
\end{table}

In this work we use the AlexNet trained on Places-365 everywhere unless explicitly mentioned that the AlexNet trained on ImageNet is used.

\subsubsection{Which layer and metric to choose?}
To find the baseline image, we employ the following method. We use the Nx1 dimensional vector produced by the network. As we go deeper into the layers, we expect more semantic features to be captured. The features are represented in the form of a vector and is known as a feature vector. In the AlexNet architecture we specifically compare three layers fc7, fc8, and prob layer which is the output after the operation of softmax function.

So we intend to find a way such that given the feature vectors from the probe image, we want to be find the base image. We use a simple approach for this. We find the feature vectors of the base image as well, and then compare the feature vectors using different metrics. For a metric to be good we would ideally want for a probe base pair it should give a high value and for unrelated images it should return a very low value. Also we would prefer a substantial difference between related and unrelated images. For this work we tried the following metrics :
\begin{itemize}
\item SSD : Sum of Squared Distances
\item SAD : Sum of Absolute Distances
\item NCC : Pearson's correlation coefficient
\end{itemize}

It turned out that NCC gave the most desirable results.
\begin{figure}[H]
  \centering
  \begin{subfigure}[t]{0.45\textwidth}
    \centering
    \includegraphics[scale=0.1]{images/nc17_d1b4/pb11}
  \end{subfigure}
  % ~
  \begin{subfigure}[t]{0.45\textwidth}
    \centering
    \includegraphics[scale=0.1]{images/nc17_d1b4/pb12}
  \end{subfigure}
  \caption{Probe Base Pair taken from Nimble Dataset 2017 Dev 1 Beta 4}
  \label{fig:ncc_res}
\end{figure}

For example in the image pair \ref{fig:ncc_res} the metrics for the prob layers were as follows :

\begin{table}[H]
\centering
\caption{Prob Layer Metrics}
\label{pb_metrics}
\begin{tabular}{|l|l|}
  \hline
  & Prob layer          \\
  \hline
  SSD & 0.072518922         \\
  SAD & 0.29026049          \\
  NCC & 0.98303729249860383 \\
  \hline
\end{tabular}
\end{table}

Clearly SAD is not desirable since it gives a medium score to a matching pair. Both SSD and NCC give good results in this case, but emperically it was found that NCC is not only easier for comparison (need not invert the high and low score), but is also more robust, that is gives high score even in cases where the images have been manipulated to larger degree and SSD isn't able to caputre the similarity. As a result, NCC has been the prime candidate for the rest of the work.

Another important part was to choose a layer. It was found that for many cases that using the last layer (prob) gave a very low score for a matching pair.

\begin{figure}[H]
  \centering
  \begin{subfigure}[t]{0.5\textwidth}
    \centering
    \includegraphics[scale=0.05]{images/nc17_d1b4/pb21}
  \end{subfigure}
  % ~
  \begin{subfigure}[t]{0.45\textwidth}
    \centering
    \includegraphics[scale=0.05]{images/nc17_d1b4/pb22}
  \end{subfigure}
  \caption{Probe Base Pair taken from Nimble Dataset 2017 Dev 1 Beta 4}
  \label{fig:fc8_good}
\end{figure}

For example the image pair \ref{fig:fc8_good} returns the following NCC scores for the three layers.
\begin{table}[H]
\centering
\caption{NCC scores for different layers}
\label{my-label}
\begin{tabular}{|l|l|}
  \hline
  & NCC                 \\
  \hline
  fc7  & 0.75791334934860821 \\
  fc8  & 0.87673938938958917 \\
  prob & 0.35012885670990512 \\
  \hline
\end{tabular}
\end{table}

Clearly fc8 gives the most desirable result, but it is interesting to theorize the reason why prob gives such a low score. We hypothesize that the SoftMax layer in some sense disturbs the features because it gives the probability of closeness to a particular scene. So if a scene is not present in the Places365 database, this would give a weird output. Also going by the emperical knowledge that the deeper layers tend to extract out more semantic features, fc8 should give the best result and this intuition follows our finding. fc8 gives consistently higher score than fc7 and fc6 for probe base pair and lower score for unrelated images.

Here is a small table comparing the different methods

\begin{table}[H]
\centering
\caption{Layers and Correlation threshold on the Nimble 2016 dataset which were known to be manipulated}
\label{lct}
\begin{tabular}{|l|l|l|l|l|ll|}
  \hline
  Tot Images = 320 & \multicolumn{2}{|l|}{Prob} & \multicolumn{2}{|l|}{fc8} & \multicolumn{2}{|l|}{fc7} \\
  \hline
  \multicolumn{1}{|l|}{Threshold} & correct    & \%correct   & correct   & \%correct   & correct   & \%correct   \\
  \hline
  0.95             & 203        & 0.634375    & 271       & 0.846875    & 175       & 0.546875    \\
  0.9              & 243        & 0.759375    & 288       & 0.9         & 243       & 0.759375    \\
  0.8              & 264        & 0.825       & 312       & 0.975       & 287       & 0.896875    \\
  0.5              & 295        & 0.921875    & 316       & 0.9875      & 313       & 0.978125    \\
  0.4              & 302        & 0.94375     & 320       & 1           & 316       & 0.9875      \\
  \hline

\end{tabular}
\end{table}

\subsubsection{Speeding up the Feature Extraction Process}
% Add a few lines about the multiprocessing library

\subsubsection{Image Slicing}
A general observation in the datasets was that the manipulation existed in only a part of the probe image. For this reason, we use the method of image slicing, that is cutting the image into two halves horizontally or vertically, even getting four quadrants as well. Then we match each slice with the corresponding slice in the other image. This gives a very easy boost to the accuracy but at the same time demands more computational resources or time.

Here is the table for layer fc8 with horizontal slicing:

\begin{table}[H]
\centering
\caption{Increase in accuracy using slicing}
\label{slicing}
\begin{tabular}{|l|l|l|}
  \hline
  Tot Images = 320 & \multicolumn{2}{|l|}{fc8 sliced} \\
  \hline
  Threshold        & correct       & \%correct      \\
  \hline
  0.95             & 203           & 0.634375       \\
  0.9              & 243           & 0.759375       \\
  0.8              & 264           & 0.825          \\
  0.5              & 295           & 0.921875       \\
  0.4              & 302           & 0.94375 \\
  \hline
\end{tabular}
\end{table}

Some problems with this method include that it is not able to identify if the image has been rotated. This also fails miserably if the donor image occupies a significant (more than 70\%) of the whole image.

\subsubsection{Clustering using K-means}
A detour atttempt was made to check how far the correlation matching could take us. We simply tried using the feature vectors derived from all the probe image which were known to be manipulated and all the world images from the Nimble 2017 Dev 1 Beta 4 dataset and tried to use a k-means clustering implemented in scikit-learn \cite{scikit-learn}. In the dataset there were 65 probe images which were known to be manipulated, and hence k in k-means was chosen to be 65. The number of iterations was kept to 100 but changing to 1000 or even higher didn't change the actual result. This table summarizes the findings :

\begin{table}[H]
\centering
\caption{Clustering observation}
\label{my-label}
\begin{tabular}{|l|l|}
  \hline
  Cluster observed    & Number of clusters \\
  \hline
  Very good              & 37                 \\
  one bad             & 10                 \\
  two bad             & 3                  \\
  two cluster overlap & 6                  \\
  bad cases           & 9                 \\
  \hline
\end{tabular}
\end{table}
Very good implies no false positives or negatives, whereas one bad and two bad imply that there is one or two false positive. Two cluster overlap implies two clusters overlapped and couldn't form distinct clusters and a possible reason this happened would be because of less number cluster centres available. Bad cases include all other cases which includes random images together, three clusters, more than two bad images etc. Here is one very good cluster

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.35]{images/cluster_1}
  \caption{Cluster 1 form NC2017 Dev 1 Beta 4 dataset}
  \label{fig:cluster}
\end{figure}

% A significant difficulty with this approach is that
\subsubsection{Getting all the Base Images}
The Nimble Dataset 2017 Dev 3 Beta 1 had a lot more probe and world images, specifically 2157 manipulated probe images and 4098 world images. The world set not only consisted of the base image, but also of the probe image as well as the intermediate manipulated images. This essentially means that there is one particular base image and then subsequent manipulations on the top of that base image gives us the probe image. We aim to find all the manipulated images along with the base image.

For this we use a graph based approach. All graphs are made using networkx. We first create a graph G with all the probe images as the node. Then we add all the images in the world (discarding the probe images) to the Graph and create an edge with all the nodes, with the weight of the edge as the correlation between the two images. We then start with one probe image, and then look at the edge with highest weight. We now contract the two nodes into one, and in this processs recompute the correlation taking the maximum of the two correlations. Then we repeat the process. The termination step is not exactly defined and for now we terminate based on the existing knowledge of the number of matches that should have occured (using the ground truth data). We then simply repeat this process for the rest of the probe images (initializing the Graph as well).

We use both Alexnet trained on Places365 as well as ImageNet. This method is henceforth referred to as recurrent base detection.

\begin{table}[H]
\centering
\caption{Recurrent Base Detection}
\label{recbd}
\begin{tabular}{|l|l|l|l|l|}
\hline
                                                                                                 & \multicolumn{2}{l|}{Alexnet on Places} & \multicolumn{2}{l|}{Alexnet on ImageNet} \\ \hline
No. of probe images                                                                              & 2157              &                    & 2157               &                     \\ \hline
\begin{tabular}[c]{@{}l@{}}No. of probe images \\ with all baseline\\ correct match\end{tabular} & 1120              &                    & 1357               &                     \\ \hline
                                                                                                 & 1120/2157         & 0.5192396847       & 1357/2157          & 0.6291145109        \\ \hline
Total no. of base images                                                                         & 56223             &                    & 56223              &                     \\ \hline
\begin{tabular}[c]{@{}l@{}}No. of base images \\ correctly identified\end{tabular}               & 48732             &                    & 49974              &                     \\ \hline
                                                                                                 & 48732/56223       & 0.8667627128       & 49974/56223        & 0.8888533163        \\ \hline
\end{tabular}
\end{table}

The first set of rows define the number of probe image with correct matches. We define a probe to be correctly matched if and only if all the manipulations were successfully captured. As can be seen this number is on the lower side. The second set of rows define the number of base images correctly recognised. That is for a particular probe image it is likely that there 7 correct images identified and 3 incorrect, even though this would make the probe image be an incorrect match, it would still be counted as 7 correct matches for the base images. We note that this is moderately on the higher side 85-89\%.

It is quite interesting to note that the AlexNet trained on ImageNet outperforms the AlexNet trained on Places365. This is probably because the manipulations in the Nimble Dataset 2017 Dev 3 Beta 1 involved small manipulations.



\subsection{Donor Detection}

\section{Troublesome Cases}
Here are some cases, for which we are clueless as to why the correlation is low.

\bibliography{./ref.bib}
\bibliographystyle{ieeetr}

\end{document}
